{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Project (Labelled Unreliable News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, path='./raw_data/fulltrain.csv', remove_punctuation=False):\n",
    "        # Read CSV file in\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        self.df = df\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "\n",
    "        # Stopwords\n",
    "        self.stopwords_set = set(stopwords.words('english'))\n",
    "        # Vocabulary sets - Just lists the unique\n",
    "        self.satire_vocabulary = set()\n",
    "        self.hoax_vocabulary = set()\n",
    "        self.propaganda_vocabulary = set()\n",
    "        self.reliable_vocabulary = set()\n",
    "\n",
    "        self.satire_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.hoax_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.propaganda_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.reliable_vocab_dictionary = defaultdict(lambda: 0)\n",
    "\n",
    "    def get_data(self):\n",
    "        df = self.df\n",
    "        label_counts = self.count_labels()\n",
    "\n",
    "        # Total number of words\n",
    "        raw_word_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        # Total number of words without stopwords\n",
    "        stopword_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        \n",
    "        # Do the actual data processing\n",
    "        for i, row in df.iterrows():\n",
    "            print('Parsing row number: ' + str(i), end = \"\\r\")\n",
    "            label = int(row[0])\n",
    "            text = row[1]\n",
    "            # List of tokens for the sentence\n",
    "            tokens = self.tokenise_text(text)\n",
    "            raw_word_count_dict[label] += len(tokens)\n",
    "\n",
    "            # Will filter out all punctuation - Punctuation is important in this context (!, ? has sentiment)\n",
    "            if self.remove_punctuation:\n",
    "                depunctuated_tokens = []\n",
    "                for token in tokens:\n",
    "                    if re.match(r'^[\\W|_]*$', token):\n",
    "                        continue\n",
    "                    depunctuated_tokens.append(token)\n",
    "                tokens = depunctuated_tokens\n",
    "\n",
    "            # Can think of doing normalisation/stemming over here before removing stopwords\n",
    "            # POS tagging - Noun/Pronoun. Looking at 3rd person/1st person POV words\n",
    "\n",
    "            # Count and remove stopwords\n",
    "            count_and_remove_stopword_result = self.count_and_remove_stopwords(tokens)\n",
    "            stopword_count_dict[label] += count_and_remove_stopword_result[0]\n",
    "            tokens_no_stopwords = count_and_remove_stopword_result[1]\n",
    "            self.update_vocabulary(label, tokens_no_stopwords)\n",
    "\n",
    "        # Print out all the acquired data\n",
    "        print()\n",
    "        print('Raw total count')\n",
    "        print(raw_word_count_dict)\n",
    "        print('Raw stopword count')\n",
    "        print(stopword_count_dict)\n",
    "        print()\n",
    "        # Calculate averages\n",
    "        for key, value in label_counts.items():\n",
    "            raw_word_count_dict[key] /= value\n",
    "            stopword_count_dict[key] /= value\n",
    "        print('Average number of words')\n",
    "        print(raw_word_count_dict)\n",
    "        print('Average number of stop words')\n",
    "        print(stopword_count_dict)\n",
    "\n",
    "        print()\n",
    "        print('Unique vocabulary count: ')\n",
    "        print('Satire:' + str(len(self.satire_vocabulary)))\n",
    "        print('Hoax:' +str(len(self.hoax_vocabulary)))\n",
    "        print('Propaganda:' + str(len(self.propaganda_vocabulary)))\n",
    "        print('Reliable:' +str(len(self.reliable_vocabulary)))\n",
    "        print()\n",
    "        print('Sorting dictionaries...')\n",
    "        sorted_satire = sorted(self.satire_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_hoax = sorted(self.hoax_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_propaganda = sorted(self.propaganda_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_reliable = sorted(self.reliable_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        print('Sorting done!')\n",
    "        # Get top 50 most common words\n",
    "        sorted_satire = sorted_satire[:50]\n",
    "        sorted_hoax = sorted_hoax[:50]\n",
    "        sorted_propaganda = sorted_propaganda[:50]\n",
    "        sorted_reliable = sorted_reliable[:50]\n",
    "        print('Satire:')\n",
    "        print(sorted_satire)\n",
    "        print('Hoax:')\n",
    "        print(sorted_hoax)\n",
    "        print('Propaganda:')\n",
    "        print(sorted_propaganda)\n",
    "        print('Reliable:')\n",
    "        print(sorted_reliable)\n",
    "    \n",
    "    # Counts the number of each label in the dataset\n",
    "    def count_labels(self):\n",
    "        df = self.df\n",
    "        print('Number of rows: ' + str(df.shape[0]))\n",
    "        label_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        for i, row in df.iterrows():\n",
    "            label = int(row[0])\n",
    "            label_count_dict[label] += 1\n",
    "            # Row 1 == Text\n",
    "            # print(f\"{row[1]}\")\n",
    "        print('SATIRE, HOAX, PROPAGANDA, RELIABLE NEWS')\n",
    "        print(label_count_dict)\n",
    "        return label_count_dict\n",
    "\n",
    "    # Updates a set of unique vocabulary\n",
    "    def update_vocabulary(self, label, tokens):\n",
    "        if label == 1:\n",
    "            self.satire_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.satire_vocab_dictionary[token] += 1\n",
    "        elif label == 2:\n",
    "            self.hoax_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.hoax_vocab_dictionary[token] += 1\n",
    "        elif label == 3:\n",
    "            self.propaganda_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.propaganda_vocab_dictionary[token] += 1\n",
    "        elif label == 4:\n",
    "            self.reliable_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.reliable_vocab_dictionary[token] += 1\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Returns number of stopwords in a sentence and the set of tokens without stopwords. Uses NLTK stopwords library\n",
    "    def count_and_remove_stopwords(self, tokens):\n",
    "        stopword_count = 0\n",
    "        new_token_list = []\n",
    "        for token in tokens:\n",
    "            if token in self.stopwords_set:\n",
    "                stopword_count += 1\n",
    "                continue\n",
    "            new_token_list.append(token)\n",
    "        return (stopword_count, new_token_list)\n",
    "    \n",
    "    # Returns a list of tokens - Utilises NLTK tokenizer\n",
    "    def tokenise_text(self, text):\n",
    "        raw_text = text.lower()\n",
    "        # raw_text = 'Mrs. Brown said, \"But where are you going now?\" ... <h3> U.S.A'\n",
    "        return nltk.word_tokenize(raw_text)\n",
    "\n",
    "    def test_function(self):\n",
    "        raw_text = 'Mrs. Brown said that she will bring us to hear his speech whose who it their they ... <h3> U.S.A'\n",
    "        # raw_text = 'The the the guys said that it is what it is haha! haha haha haha What are you doing today?'\n",
    "        tokens = self.tokenise_text(raw_text)\n",
    "        print(tokens)\n",
    "        # Method 1 - Will not remove words like mrs. u.s.a\n",
    "        depunctuated_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.match(r'^[\\W|_]*$', token):\n",
    "                continue\n",
    "            depunctuated_tokens.append(token)\n",
    "        print('DEPUNCTUATED')\n",
    "        print(depunctuated_tokens)\n",
    "\n",
    "        \n",
    "        tagged_tokens = nltk.pos_tag(depunctuated_tokens)\n",
    "        print(tagged_tokens)\n",
    "        for token in tagged_tokens:\n",
    "            tag = token[1]\n",
    "\n",
    "\n",
    "        # Method 2: Will remove the above words        \n",
    "        # depunctuated_tokens = [word for word in tokens if word.isalpha()]\n",
    "        result = self.count_and_remove_stopwords(depunctuated_tokens)\n",
    "        print('NO MORE STOP WORDS')\n",
    "        print(result[0])\n",
    "        print(result[1])\n",
    "        # print(self.stopwords_set)\n",
    "\n",
    "        print()\n",
    "        test_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        for word in result[1]:\n",
    "            test_vocab_dictionary[word] += 1\n",
    "        print(sorted(test_vocab_dictionary.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mrs.', 'brown', 'said', 'that', 'she', 'will', 'bring', 'us', 'to', 'hear', 'his', 'speech', 'whose', 'who', 'it', 'their', 'they', '...', '<', 'h3', '>', 'u.s.a']\n",
      "DEPUNCTUATED\n",
      "['mrs.', 'brown', 'said', 'that', 'she', 'will', 'bring', 'us', 'to', 'hear', 'his', 'speech', 'whose', 'who', 'it', 'their', 'they', 'h3', 'u.s.a']\n",
      "[('mrs.', 'NNS'), ('brown', 'VBN'), ('said', 'VBD'), ('that', 'IN'), ('she', 'PRP'), ('will', 'MD'), ('bring', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('hear', 'VB'), ('his', 'PRP$'), ('speech', 'NN'), ('whose', 'WP$'), ('who', 'WP'), ('it', 'PRP'), ('their', 'PRP$'), ('they', 'PRP'), ('h3', 'VBP'), ('u.s.a', 'JJ')]\n",
      "NO MORE STOP WORDS\n",
      "9\n",
      "['mrs.', 'brown', 'said', 'bring', 'us', 'hear', 'speech', 'whose', 'h3', 'u.s.a']\n",
      "\n",
      "[('mrs.', 1), ('brown', 1), ('said', 1), ('bring', 1), ('us', 1), ('hear', 1), ('speech', 1), ('whose', 1), ('h3', 1), ('u.s.a', 1)]\n"
     ]
    }
   ],
   "source": [
    "news_classifier = NewsClassifier(remove_punctuation=True)\n",
    "# news_classifier.get_data()\n",
    "news_classifier.test_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b11488b760cad6b42c613d6262757ba6dddd45f4ba0ba03a1dc94b877a66cfb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
