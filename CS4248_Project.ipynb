{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Project (Labelled Unreliable News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, path='./raw_data/fulltrain.csv', remove_punctuation=False):\n",
    "        # Read CSV file in\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        self.df = df\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "\n",
    "        # Stopwords\n",
    "        self.stopwords_set = set(stopwords.words('english'))\n",
    "        # Vocabulary sets - Just lists the unique\n",
    "        self.satire_vocabulary = set()\n",
    "        self.hoax_vocabulary = set()\n",
    "        self.propaganda_vocabulary = set()\n",
    "        self.reliable_vocabulary = set()\n",
    "\n",
    "        self.satire_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.hoax_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.propaganda_vocab_dictionary = defaultdict(lambda: 0)\n",
    "        self.reliable_vocab_dictionary = defaultdict(lambda: 0)\n",
    "\n",
    "        self.satire_pronouns_dictionary = {'Personal': 0, 'WH-Pronoun': 0, 'Total': 0}\n",
    "        self.hoax_pronouns_dictionary = {'Personal': 0, 'WH-Pronoun': 0, 'Total': 0}\n",
    "        self.propaganda_pronouns_dictionary = {'Personal': 0, 'WH-Pronoun': 0, 'Total': 0}\n",
    "        self.reliable_pronouns_dictionary = {'Personal': 0, 'WH-Pronoun': 0, 'Total': 0}\n",
    "\n",
    "    def get_data(self):\n",
    "        df = self.df\n",
    "        label_counts = self.count_labels()\n",
    "\n",
    "        # Total number of words\n",
    "        raw_word_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        # Total number of words without stopwords\n",
    "        stopword_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        \n",
    "        # Do the actual data processing\n",
    "        for i, row in df.iterrows():\n",
    "            print('Parsing row number: ' + str(i), end = \"\\r\")\n",
    "            label = int(row[0])\n",
    "            text = row[1]\n",
    "            # List of tokens for the sentence\n",
    "            tokens = self.tokenise_text(text)\n",
    "            raw_word_count_dict[label] += len(tokens)\n",
    "\n",
    "            # Will filter out all punctuation - Punctuation is important in this context (!, ? has sentiment)\n",
    "            if self.remove_punctuation:\n",
    "                depunctuated_tokens = []\n",
    "                for token in tokens:\n",
    "                    if re.match(r'^[\\W|_]*$', token):\n",
    "                        continue\n",
    "                    depunctuated_tokens.append(token)\n",
    "                tokens = depunctuated_tokens\n",
    "\n",
    "            # Can think of doing normalisation/stemming over here before removing stopwords\n",
    "            # POS tagging - Noun/Pronoun. Expand: Looking at 3rd person/1st person POV words\n",
    "            tagged_tokens = nltk.pos_tag(tokens)\n",
    "            self.count_pronouns(tagged_tokens, label)\n",
    "\n",
    "            # Count and remove stopwords\n",
    "            count_and_remove_stopword_result = self.count_and_remove_stopwords(tokens)\n",
    "            stopword_count_dict[label] += count_and_remove_stopword_result[0]\n",
    "            tokens_no_stopwords = count_and_remove_stopword_result[1]\n",
    "            self.update_vocabulary(label, tokens_no_stopwords)\n",
    "\n",
    "        # Print out all the acquired data\n",
    "        print()\n",
    "        print('Raw total count')\n",
    "        print(raw_word_count_dict)\n",
    "        print('Raw stopword count')\n",
    "        print(stopword_count_dict)\n",
    "        print()\n",
    "        # Calculate averages\n",
    "        for key, value in label_counts.items():\n",
    "            raw_word_count_dict[key] /= value\n",
    "            stopword_count_dict[key] /= value\n",
    "        print('Average number of words')\n",
    "        print(raw_word_count_dict)\n",
    "        print('Average number of stop words')\n",
    "        print(stopword_count_dict)\n",
    "\n",
    "        print()\n",
    "        print('=== Unique vocabulary count: ===')\n",
    "        print('Satire:' + str(len(self.satire_vocabulary)))\n",
    "        print('Hoax:' +str(len(self.hoax_vocabulary)))\n",
    "        print('Propaganda:' + str(len(self.propaganda_vocabulary)))\n",
    "        print('Reliable:' +str(len(self.reliable_vocabulary)))\n",
    "        print()\n",
    "        print('Sorting dictionaries...')\n",
    "        sorted_satire = sorted(self.satire_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_hoax = sorted(self.hoax_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_propaganda = sorted(self.propaganda_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_reliable = sorted(self.reliable_vocab_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "        print('Sorting done!')\n",
    "        # Get top 50 most common words\n",
    "        sorted_satire = sorted_satire[:50]\n",
    "        sorted_hoax = sorted_hoax[:50]\n",
    "        sorted_propaganda = sorted_propaganda[:50]\n",
    "        sorted_reliable = sorted_reliable[:50]\n",
    "        print('=== Top 50 most common words: ===')\n",
    "        print('Satire:')\n",
    "        print(sorted_satire)\n",
    "        print('Hoax:')\n",
    "        print(sorted_hoax)\n",
    "        print('Propaganda:')\n",
    "        print(sorted_propaganda)\n",
    "        print('Reliable:')\n",
    "        print(sorted_reliable)\n",
    "        print()\n",
    "        print('=== Pronoun Counts (Averages): ===')\n",
    "        print('Satire:')\n",
    "        print(self.satire_pronouns_dictionary)\n",
    "        print(self.satire_pronouns_dictionary['Personal'] / label_counts[1])\n",
    "        print(self.satire_pronouns_dictionary['WH-Pronoun'] / label_counts[1])\n",
    "        print(self.satire_pronouns_dictionary['Total'] / label_counts[1])\n",
    "        print('Hoax:')\n",
    "        print(self.hoax_pronouns_dictionary)\n",
    "        print(self.hoax_pronouns_dictionary['Personal'] / label_counts[2])\n",
    "        print(self.hoax_pronouns_dictionary['WH-Pronoun'] / label_counts[2])\n",
    "        print(self.hoax_pronouns_dictionary['Total'] / label_counts[2])\n",
    "        print('Propaganda:')\n",
    "        print(self.propaganda_pronouns_dictionary)\n",
    "        print(self.propaganda_pronouns_dictionary['Personal'] / label_counts[3])\n",
    "        print(self.propaganda_pronouns_dictionary['WH-Pronoun'] / label_counts[3])\n",
    "        print(self.propaganda_pronouns_dictionary['Total'] / label_counts[3])\n",
    "        print('Reliable:')\n",
    "        print(self.reliable_pronouns_dictionary)\n",
    "        print(self.reliable_pronouns_dictionary['Personal'] / label_counts[4])\n",
    "        print(self.reliable_pronouns_dictionary['WH-Pronoun'] / label_counts[4])\n",
    "        print(self.reliable_pronouns_dictionary['Total'] / label_counts[4])\n",
    "    \n",
    "    # Counts the number of each label in the dataset\n",
    "    def count_labels(self):\n",
    "        df = self.df\n",
    "        print('Number of rows: ' + str(df.shape[0]))\n",
    "        label_count_dict = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        for i, row in df.iterrows():\n",
    "            label = int(row[0])\n",
    "            label_count_dict[label] += 1\n",
    "            # Row 1 == Text\n",
    "            # print(f\"{row[1]}\")\n",
    "        print('SATIRE, HOAX, PROPAGANDA, RELIABLE NEWS')\n",
    "        print(label_count_dict)\n",
    "        return label_count_dict\n",
    "\n",
    "    # Returns the 2 different counts of pronouns. Receives a list of tagged_tokens (tuples)\n",
    "    def count_pronouns(self, tagged_tokens, label):\n",
    "        # PRP personal pronoun I, he, she \n",
    "        # PRP$ possessive pronoun my, his, hers\n",
    "        # WP wh-pronoun who, what\n",
    "        # WP$ possessive wh-pronoun whose\n",
    "        personal_pronoun_list = ['PRP', 'PRP$']\n",
    "        personal_pronoun_set = set(personal_pronoun_list)\n",
    "        wh_pronoun_list = ['WP', 'WP$']\n",
    "        wh_pronoun_set = set(wh_pronoun_list)\n",
    "\n",
    "        personal_pronoun_count = 0\n",
    "        wh_pronoun_count = 0\n",
    "        for token in tagged_tokens:\n",
    "            tag = token[1]\n",
    "            if tag in personal_pronoun_set:\n",
    "                personal_pronoun_count += 1\n",
    "            elif tag in wh_pronoun_set:\n",
    "                wh_pronoun_count += 1\n",
    "        total_count = personal_pronoun_count + wh_pronoun_count\n",
    "\n",
    "        if label == 1:\n",
    "            self.satire_pronouns_dictionary['Personal'] += personal_pronoun_count\n",
    "            self.satire_pronouns_dictionary['WH-Pronoun'] += wh_pronoun_count\n",
    "            self.satire_pronouns_dictionary['Total'] += total_count\n",
    "        elif label == 2:\n",
    "            self.hoax_pronouns_dictionary['Personal'] += personal_pronoun_count\n",
    "            self.hoax_pronouns_dictionary['WH-Pronoun'] += wh_pronoun_count\n",
    "            self.hoax_pronouns_dictionary['Total'] += total_count\n",
    "        elif label == 3:\n",
    "            self.propaganda_pronouns_dictionary['Personal'] += personal_pronoun_count\n",
    "            self.propaganda_pronouns_dictionary['WH-Pronoun'] += wh_pronoun_count\n",
    "            self.propaganda_pronouns_dictionary['Total'] += total_count\n",
    "        elif label == 4:\n",
    "            self.reliable_pronouns_dictionary['Personal'] += personal_pronoun_count\n",
    "            self.reliable_pronouns_dictionary['WH-Pronoun'] += wh_pronoun_count\n",
    "            self.reliable_pronouns_dictionary['Total'] += total_count\n",
    "        pass\n",
    "\n",
    "    # Updates a set of unique vocabulary\n",
    "    def update_vocabulary(self, label, tokens):\n",
    "        if label == 1:\n",
    "            self.satire_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.satire_vocab_dictionary[token] += 1\n",
    "        elif label == 2:\n",
    "            self.hoax_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.hoax_vocab_dictionary[token] += 1\n",
    "        elif label == 3:\n",
    "            self.propaganda_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.propaganda_vocab_dictionary[token] += 1\n",
    "        elif label == 4:\n",
    "            self.reliable_vocabulary.update(set(tokens))\n",
    "            for token in tokens:\n",
    "                self.reliable_vocab_dictionary[token] += 1\n",
    "        pass\n",
    "\n",
    "    # Returns number of stopwords in a sentence and the set of tokens without stopwords. Uses NLTK stopwords library\n",
    "    def count_and_remove_stopwords(self, tokens):\n",
    "        stopword_count = 0\n",
    "        new_token_list = []\n",
    "        for token in tokens:\n",
    "            if token in self.stopwords_set:\n",
    "                stopword_count += 1\n",
    "                continue\n",
    "            new_token_list.append(token)\n",
    "        return (stopword_count, new_token_list)\n",
    "    \n",
    "    # Returns a list of tokens - Utilises NLTK tokenizer. Set to lowercase\n",
    "    def tokenise_text(self, text):\n",
    "        raw_text = text.lower()\n",
    "        return nltk.word_tokenize(raw_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 48854\n",
      "SATIRE, HOAX, PROPAGANDA, RELIABLE NEWS\n",
      "{1: 14047, 2: 6942, 3: 17870, 4: 9995}\n",
      "Parsing row number: 48853\n",
      "Raw total count\n",
      "{1: 4756605, 2: 1503599, 3: 18189450, 4: 5052139}\n",
      "Raw stopword count\n",
      "{1: 1722674, 2: 605984, 3: 7131322, 4: 1786363}\n",
      "\n",
      "Average number of words\n",
      "{1: 338.62070192923755, 2: 216.5944972630366, 3: 1017.876329043089, 4: 505.4666333166583}\n",
      "Average number of stop words\n",
      "{1: 122.63643482594148, 2: 87.29242293287237, 3: 399.06670397313934, 4: 178.72566283141572}\n",
      "\n",
      "=== Unique vocabulary count: ===\n",
      "Satire:120202\n",
      "Hoax:36659\n",
      "Propaganda:217713\n",
      "Reliable:118406\n",
      "\n",
      "Sorting dictionaries...\n",
      "Sorting done!\n",
      "=== Top 50 most common words: ===\n",
      "Satire:\n",
      "[('said', 34913), (\"'s\", 34890), (\"n't\", 11049), ('one', 10526), ('would', 10226), ('time', 10110), ('like', 9006), ('new', 7337), ('could', 6669), ('added', 6300), ('even', 6260), ('get', 6092), ('people', 5546), ('amp', 5240), ('monday', 5064), ('also', 5043), ('years', 4801), ('know', 4771), ('really', 4748), ('first', 4745), ('back', 4537), ('day', 4393), ('told', 4364), ('going', 4348), ('according', 4346), ('last', 4239), ('every', 4181), ('many', 4176), ('way', 4059), ('two', 4024), ('sources', 4017), ('make', 3959), ('right', 3923), ('reportedly', 3799), ('president', 3701), ('man', 3671), ('never', 3596), ('reporters', 3581), ('still', 3545), ('go', 3517), ('press', 3491), ('see', 3470), ('around', 3436), (\"'m\", 3426), ('much', 3404), ('confirmed', 3362), ('think', 3347), ('say', 3304), (\"'the\", 3275), ('little', 3260)]\n",
      "Hoax:\n",
      "[('obama', 7050), ('think', 6184), ('trump', 4860), ('one', 3522), ('president', 3350), ('according', 3192), ('video', 2997), ('people', 2873), ('would', 2766), ('reports', 2764), ('time', 2497), ('told', 2309), ('country', 2207), ('also', 2178), ('clinton', 2165), ('like', 2150), ('recent', 2128), ('new', 2124), ('said', 2111), ('us', 2109), ('hillary', 2050), ('american', 1966), ('white', 1927), ('made', 1884), ('house', 1819), ('support', 1772), ('first', 1771), ('news', 1742), ('many', 1686), ('donald', 1684), ('take', 1637), ('states', 1631), ('obamas', 1596), ('state', 1571), ('going', 1554), ('law', 1485), ('claims', 1473), ('could', 1458), ('watch', 1458), ('get', 1419), ('two', 1401), ('police', 1396), ('years', 1362), ('man', 1352), ('make', 1349), ('back', 1341), ('know', 1333), ('muslim', 1331), ('dont', 1330), ('americans', 1321)]\n",
      "Propaganda:\n",
      "[('people', 42022), ('us', 40875), ('would', 38999), ('one', 38322), ('government', 35508), ('like', 27281), ('also', 24880), ('new', 24774), ('even', 24435), ('world', 23399), ('time', 21937), ('many', 21741), ('said', 19482), ('state', 19398), ('could', 19234), ('years', 18038), ('states', 18015), ('u.s.', 17336), ('get', 17180), ('first', 17164), ('dont', 16665), ('war', 16143), ('know', 16139), ('well', 16114), ('may', 15919), ('article', 15243), ('see', 15055), ('health', 14699), ('make', 14585), ('way', 14205), ('use', 14171), ('public', 14167), ('much', 13891), ('american', 13888), ('system', 13566), ('police', 13372), ('law', 13314), ('right', 13222), ('media', 13175), ('united', 12752), ('going', 12704), ('money', 12685), ('information', 12613), ('power', 12459), ('since', 12437), ('take', 12379), ('food', 12257), ('think', 12105), ('used', 12010), ('military', 11864)]\n",
      "Reliable:\n",
      "[(\"'s\", 48789), ('said', 38603), ('would', 10467), ('percent', 9283), ('one', 9065), ('new', 8821), ('also', 8220), (\"n't\", 8184), ('year', 7827), ('people', 7776), ('two', 7338), ('taiwan', 6990), ('government', 6712), ('first', 6142), ('last', 5906), ('years', 5787), ('president', 5685), ('could', 5564), ('time', 5408), ('china', 5388), ('u.s.', 4855), ('like', 4689), ('million', 4649), ('world', 4630), (\"'the\", 4582), ('many', 4259), ('us', 4123), ('state', 4093), ('country', 3993), ('according', 3900), ('three', 3748), ('city', 3549), ('since', 3539), ('even', 3514), (\"'we\", 3454), ('made', 3453), ('officials', 3444), ('get', 3413), ('billion', 3379), ('make', 3350), ('day', 3321), ('national', 3311), ('may', 3269), ('told', 3247), ('obama', 3242), ('states', 3216), ('united', 3201), ('public', 3128), ('still', 3113), ('back', 3065)]\n",
      "\n",
      "=== Pronoun Counts (Averages): ===\n",
      "Satire:\n",
      "15.872072328611091\n",
      "1.8224531928525665\n",
      "17.694525521463657\n",
      "Hoax:\n",
      "12.578363583981561\n",
      "1.64577931431864\n",
      "14.224142898300201\n",
      "Propaganda:\n",
      "42.315724678231675\n",
      "5.002630106323447\n",
      "47.31835478455512\n",
      "Reliable:\n",
      "16.766383191595796\n",
      "2.1032516258129066\n",
      "18.869634817408706\n"
     ]
    }
   ],
   "source": [
    "news_classifier = NewsClassifier(remove_punctuation=True)\n",
    "news_classifier.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b11488b760cad6b42c613d6262757ba6dddd45f4ba0ba03a1dc94b877a66cfb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
