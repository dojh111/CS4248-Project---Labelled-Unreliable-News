{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Pre-process training and testing datasets and save them as CSV files to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file in\n",
    "train_path = './raw_data/fulltrain.csv'\n",
    "test_path = './raw_data/balancedtest.csv'\n",
    "df = pd.read_csv(train_path, header=None)\n",
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "print('Total rows, Total Columns: ' + str(df.shape))\n",
    "df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total rows, Total Columns: ' + str(test_df.shape))\n",
    "test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Create set of stopwords for use in preprocessing\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "# print(stopword_set)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokenised_text = nltk.word_tokenize(text)\n",
    "    # Tag with Penn Treebank POS tags\n",
    "    tagged_text = nltk.pos_tag(tokenised_text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word in tagged_text:\n",
    "        pos_tag = get_wordnet_pos(word[1])\n",
    "        if pos_tag == '':\n",
    "            continue\n",
    "        new_word = lemmatizer.lemmatize(word=word[0], pos=pos_tag)\n",
    "        lemmatized_words.append(new_word)\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Convert into wordnet compatible POS tags (j, v, n , a)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Fold to lower case\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenise_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text, stopword_set):\n",
    "    # Split by whitespace\n",
    "    split_text = text.split()\n",
    "    new_tokens = []\n",
    "    for token in split_text:\n",
    "        if token in stopword_set:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    # Parse back into text\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "# Remove all punctuations - Affects words such as U.S.A etc\n",
    "# Removal of stop words has to be done prior to punctuation removal\n",
    "def remove_punctuation(text):\n",
    "    depunctuated_text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    return depunctuated_text\n",
    "\n",
    "# Prevent concatenation of statistics and names\n",
    "def replace_hyphens(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "# Combine all processes into a single preprocess text function to call on df\n",
    "# Default one used for training\n",
    "def preprocess_text(text):\n",
    "    dehyphenated_text = replace_hyphens(text)\n",
    "    lowered_text = to_lower_case(dehyphenated_text)\n",
    "    initial_stopword_pass = remove_stopwords(lowered_text, stopword_set)\n",
    "    tokens = tokenise_text(initial_stopword_pass)\n",
    "    tokenised_text = ' '.join(tokens)\n",
    "    depunctuated_text = remove_punctuation(tokenised_text)\n",
    "    second_stopword_pass = remove_stopwords(depunctuated_text, stopword_set)\n",
    "    return second_stopword_pass\n",
    "\n",
    "# Testing dataset 1\n",
    "def preprocess_text_keep_punctuation(text):\n",
    "    lowered_text = to_lower_case(text)\n",
    "    initial_stopword_pass = remove_stopwords(lowered_text, stopword_set)\n",
    "    return initial_stopword_pass\n",
    "\n",
    "def remove_stopwords_two(text, stopword_set):\n",
    "    # Split by whitespace\n",
    "    split_text = text.split()\n",
    "    new_tokens = []\n",
    "    for token in split_text:\n",
    "        temp_token = token.lower()\n",
    "        if temp_token in stopword_set:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    # Parse back into text\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "# Testing dataset 2\n",
    "def preprocess_text_capitalised(text):\n",
    "    dehyphenated_text = replace_hyphens(text)\n",
    "    initial_stopword_pass = remove_stopwords_two(dehyphenated_text, stopword_set)\n",
    "    tokens = tokenise_text(initial_stopword_pass)\n",
    "    tokenised_text = ' '.join(tokens)\n",
    "    depunctuated_text = remove_punctuation(tokenised_text)\n",
    "    second_stopword_pass = remove_stopwords_two(depunctuated_text, stopword_set)\n",
    "    return second_stopword_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning training text...')\n",
    "df[1] = df[1].map(preprocess_text_capitalised)\n",
    "print('Preprocessing done!')\n",
    "df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning test text...')\n",
    "test_df[1] = test_df[1].map(preprocess_text_capitalised)\n",
    "print('Preprocessing done!')\n",
    "test_df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./raw_data/capitalized_fulltrain.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./raw_data/capitalized_balancedtest.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file in\n",
    "clean_train_path = './raw_data/capitalized_fulltrain.csv'\n",
    "clean_test_path = './raw_data/capitalized_balancedtest.csv'\n",
    "clean_df = pd.read_csv(clean_train_path, header=None)\n",
    "clean_test_df = pd.read_csv(clean_test_path, header=None)\n",
    "\n",
    "print('Total rows, Total Columns: ' + str(clean_df.shape))\n",
    "clean_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total rows, Total Columns: ' + str(clean_test_df.shape))\n",
    "clean_test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_test_df[1][2649])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
