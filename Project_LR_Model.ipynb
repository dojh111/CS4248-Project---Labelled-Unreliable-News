{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression News Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, train_path='./raw_data/fulltrain.csv', test_path='./raw_data/balancedtest.csv', target_sample_size=10000, test_size=0.3):\n",
    "        # Read CSV file in\n",
    "        self.df = pd.read_csv(train_path, header=None)\n",
    "        self.test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "        self.target_sample_size = target_sample_size\n",
    "        # Rebalanced dataframe\n",
    "        self.balanced_df = ''\n",
    "        self.class_labels = [1,2,3,4]\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.test_size = test_size\n",
    "\n",
    "        # Trained LR model\n",
    "        self.model = ''\n",
    "\n",
    "    # Returns number of each class label\n",
    "    def count_classes(self, df):\n",
    "        label_counts = defaultdict(lambda: 0)\n",
    "        # Labels are first row of DF\n",
    "        labels = df[0]\n",
    "        for label in labels:\n",
    "            label_counts[label] += 1\n",
    "        return label_counts\n",
    "\n",
    "    def undersample_data(self, samples, target_size):\n",
    "        return samples.sample(target_size)\n",
    "\n",
    "    def oversample_data(self, samples, target_size):\n",
    "        return samples.sample(target_size, replace=True)\n",
    "\n",
    "    # Does over and undersampling to achieve targeted dataset size\n",
    "    def balance_datasets(self, current_counts):\n",
    "        target_size = self.target_sample_size\n",
    "        new_samples = []\n",
    "        for label in self.class_labels:\n",
    "            samples = self.df[self.df[0] == label]\n",
    "            if current_counts[label] == target_size:\n",
    "                samples = samples\n",
    "            elif current_counts[label] > target_size:\n",
    "                # Undersample\n",
    "                samples = self.undersample_data(samples, target_size)\n",
    "            elif current_counts[label] < target_size:\n",
    "                # Oversample\n",
    "                samples = self.oversample_data(samples, target_size)\n",
    "            new_samples.append(samples)\n",
    "        print('Resampled')\n",
    "        return pd.concat(new_samples, axis=0)\n",
    "\n",
    "    # Do over and undersampling to 10k each\n",
    "    def prepare_training_data(self):\n",
    "        print('Preparing training data...')\n",
    "        label_counts = self.count_classes(self.df)\n",
    "        print(\"Label counts: \" + str(dict(label_counts)))\n",
    "        self.balanced_df = self.balance_datasets(label_counts)\n",
    "        label_counts = self.count_classes(self.balanced_df)\n",
    "        print(\"Resampled Label counts: \" + str(dict(label_counts)))\n",
    "\n",
    "    def count_and_remove_stopwords(self, tokens):\n",
    "        count = 0\n",
    "        no_stopword_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.stopwords:\n",
    "                count += 1\n",
    "                continue\n",
    "            no_stopword_tokens.append(token)\n",
    "        return (count, no_stopword_tokens)\n",
    "\n",
    "    def tag_with_pos_and_count(self, tokens):\n",
    "        pos_counts = defaultdict(lambda: 0)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        for token in tagged_tokens:\n",
    "            pos_counts[token[1]] += 1\n",
    "        return pos_counts\n",
    "\n",
    "    def identify_and_count_contractions(self, tokens):\n",
    "        contraction_pattern = re.compile(r'\\w*\\'\\w*')\n",
    "        contractions_dict = defaultdict(lambda: 0)\n",
    "        contraction_count = 0\n",
    "        for token in tokens:\n",
    "            if contraction_pattern.match(token):\n",
    "                contractions_dict[token] += 1\n",
    "                contraction_count += 1\n",
    "        return (contraction_count, contractions_dict)\n",
    "\n",
    "    def identify_and_count_pov_words(self, tokens):\n",
    "        first_pov_words = ['i', 'me', 'my', 'mine', 'myself', 'ourself', 'ourselves']\n",
    "        second_pov_words = ['you', 'yours', 'yourself', 'yourselves']\n",
    "        third_pov_words = ['he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
    "        \n",
    "        # Need to keep in order\n",
    "        pov_word_counts = { 1:0, 2:0, 3:0 }\n",
    "        for token in tokens:\n",
    "            if token in first_pov_words:\n",
    "                pov_word_counts[1] += 1\n",
    "            elif token in second_pov_words:\n",
    "                pov_word_counts[2] += 1\n",
    "            elif token in third_pov_words:\n",
    "                pov_word_counts[3] += 1\n",
    "        return pov_word_counts\n",
    "\n",
    "    # Extracts data from the input strings\n",
    "    def extract_features_from_data(self, training_data):\n",
    "        pos_tag_list = ['JJ', 'VBP', 'PRP', 'VBN', 'DT', 'NN', 'IN', 'NNS', '.', 'VBD', 'MD', 'VB', ',', 'RB', 'VBZ', 'TO', 'EX', 'CC', 'VBG', 'CD', 'WDT', 'PRP$', 'WRB', 'JJR', 'UH', 'RBR', 'WP', 'RP', ':', 'POS', 'JJS', 'NNPS', 'PDT', '``', \"''\", '$', 'RBS', 'FW', 'NNP', 'SYM', 'WP$', '(', ')']\n",
    "        contraction_list = [\"'s\", \"n't\", \"'re\", \"'ve\", \"'m\", \"'ll\", \"'d\"]\n",
    "        extracted_features = []\n",
    "\n",
    "        # Load in entire sample of texts\n",
    "        texts = training_data[1]\n",
    "\n",
    "        print('Extracting features...')\n",
    "        # Start feature extraction\n",
    "        row_count = 0\n",
    "        for text in texts:\n",
    "            print('Extracting features for number: ' + str(row_count), end = \"\\r\")\n",
    "            row_count += 1\n",
    "            features = []\n",
    "            # Initialise POS counter with list of tags to look for\n",
    "            pos_tag_counts = dict.fromkeys(pos_tag_list, 0)\n",
    "            contraction_counts = dict.fromkeys(contraction_list, 0)\n",
    "\n",
    "            # Start with tokenisation\n",
    "            text = text.lower()\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            # Feature one, length of document\n",
    "            document_length = len(tokens)\n",
    "            if document_length == 0:\n",
    "                features.append(0)\n",
    "            else:\n",
    "                features.append(math.log(len(tokens)) + 1)\n",
    "\n",
    "            stopword_count_and_remove_result = self.count_and_remove_stopwords(tokens)\n",
    "            stopword_count = stopword_count_and_remove_result[0]\n",
    "            # Token list with no stop words\n",
    "            no_stopword_tokens = stopword_count_and_remove_result[1]\n",
    "            # Add stopword_count to feature list\n",
    "            if stopword_count == 0:\n",
    "                features.append(0)\n",
    "            else:\n",
    "                features.append(math.log(stopword_count) + 1)\n",
    "\n",
    "            # Do POS tagging and counts\n",
    "            pos_counts = self.tag_with_pos_and_count(tokens)\n",
    "            for key, value in pos_counts.items():\n",
    "                if key in pos_tag_counts.keys():\n",
    "                    pos_tag_counts[key] += value\n",
    "            # Add counts of ALL tags into the feature list\n",
    "            for key, value in pos_tag_counts.items():\n",
    "                features.append(value)\n",
    "\n",
    "            # Count contractions - id:0 == count of contractions, print id:1 for dictionary of each type of contraction\n",
    "            contraction_identify_and_count_result = self.identify_and_count_contractions(tokens)\n",
    "            identified_contractions = contraction_identify_and_count_result[1]\n",
    "            for key, value in identified_contractions.items():\n",
    "                if key in contraction_counts.keys():\n",
    "                    contraction_counts[key] += value\n",
    "            # Add counts of specified contractions into the feature list\n",
    "            for key, value in contraction_counts.items():\n",
    "                features.append(value)\n",
    "\n",
    "            # Get POV word counts\n",
    "            pov_word_counts = self.identify_and_count_pov_words(tokens)\n",
    "            # Append into feature list\n",
    "            for key, value in pov_word_counts.items():\n",
    "                features.append(value)\n",
    "\n",
    "            # NO MORE FEATURES TO ADD - Append array into extracted features (2D array)\n",
    "            extracted_features.append(features)\n",
    "        print()\n",
    "        print('Feature extraction done!')\n",
    "        return extracted_features\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        model = self.model\n",
    "        print('Making predictions...')\n",
    "        for test_data in X_test:\n",
    "            data = [test_data]\n",
    "            prediction = model.predict(data)\n",
    "            predictions.append(prediction[0])\n",
    "        print('Predictions done!')\n",
    "        return predictions\n",
    "\n",
    "    def score_model(self, predictions, target_value):\n",
    "        score = f1_score(target_value, predictions, average='macro')\n",
    "        print('score on validation = {}'.format(score))\n",
    "        print()\n",
    "\n",
    "    def train_model(self):\n",
    "        self.prepare_training_data()\n",
    "        X = self.extract_features_from_data(self.balanced_df)\n",
    "        Y = self.balanced_df[0]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=self.test_size)\n",
    "\n",
    "        # Logistic Regression Model:\n",
    "        training_iterations = 3500\n",
    "        model = LogisticRegression(max_iter=training_iterations)\n",
    "        print('Training model...')\n",
    "        model.fit(X_train, y_train)\n",
    "        self.model = model\n",
    "        print('Model training done!')\n",
    "\n",
    "        # Test model\n",
    "        print('Test on training set...')\n",
    "        y_pred = self.predict(X_test)\n",
    "        self.score_model(y_pred, y_test)\n",
    "\n",
    "        # Test on 'balancedtest.csv'\n",
    "        print('Test on balanced_test.csv')\n",
    "        Y_balanced_test = self.test_df[0]\n",
    "        X_balanced_test = self.extract_features_from_data(self.test_df)\n",
    "        y_balanced_pred = self.predict(X_balanced_test)\n",
    "        self.score_model(y_balanced_pred, Y_balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Label counts: {1: 14047, 2: 6942, 3: 17870, 4: 9995}\n",
      "Resampled\n",
      "Resampled Label counts: {1: 10000, 2: 10000, 3: 10000, 4: 10000}\n",
      "Extracting features...\n",
      "Extracting features for number: 39999\n",
      "Feature extraction done!\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dojh1\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training done!\n",
      "Test on training set...\n",
      "Making predictions...\n",
      "Predictions done!\n",
      "score on validation = 0.8875446164598062\n",
      "\n",
      "Test on balanced_test.csv\n",
      "Extracting features...\n",
      "Extracting features for number: 2999\n",
      "Feature extraction done!\n",
      "Making predictions...\n",
      "Predictions done!\n",
      "score on validation = 0.3592128778019727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_classifier = NewsClassifier()\n",
    "news_classifier.train_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
