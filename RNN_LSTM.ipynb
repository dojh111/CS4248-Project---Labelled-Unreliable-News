{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM Text Classificaiton model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string                           # For removal of punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('drive/MyDrive/School Work/CS4248/News Labelling Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data into pd dataframes, data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file in\n",
    "train_path = './raw_data/fulltrain.csv'\n",
    "test_path = './raw_data/balancedtest.csv'\n",
    "df = pd.read_csv(train_path, header=None)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(df.shape))\n",
    "df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((df[0] == label).sum()))\n",
    "print(df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(test_df))\n",
    "test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((test_df[0] == label).sum()))\n",
    "print(test_df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of unique words in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique words\n",
    "def unique_word_counter(texts):\n",
    "    count = Counter() # Dictionary type\n",
    "    # Access an entire string\n",
    "    for text in texts:\n",
    "        # Split each string into individual words separated by whitespace\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run counter\n",
    "counts = unique_word_counter(df[1])\n",
    "unique_words_count = len(counts)\n",
    "print('Number of unique words: ' + str(unique_words_count))\n",
    "print('Most Common Words:')\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets, convert into numpy format for Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[1].to_numpy()\n",
    "y_train = df[0].to_numpy()\n",
    "\n",
    "X_test = test_df[1].to_numpy()\n",
    "y_test = test_df[0].to_numpy()\n",
    "\n",
    "y_train = pd.get_dummies(df[0]).values\n",
    "y_test = pd.get_dummies(test_df[0]).values\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "# y_train = y_train - 1\n",
    "# y_test = y_test - 1\n",
    "# temp_y_train = []\n",
    "# for label in y_train:\n",
    "#     temp_y_train.append([label])\n",
    "# y_train = np.array(temp_y_train)\n",
    "# temp_y_test = []\n",
    "# for label in y_test:\n",
    "#     temp_y_test.append([label])\n",
    "# y_test = np.array(temp_y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise words into numbers\n",
    "- Each word will be assigned a specific number, according to how many unique words we have\n",
    "- Inspired from this [Youtube Video](https://www.youtube.com/watch?v=kxeyoyrf2cM&ab_channel=PythonEngineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each string is turned into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=unique_words_count)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences to a common length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine minimum number of words in a sequence 65218, \n",
    "\n",
    "padding_length = 500\n",
    "\n",
    "length_count = 0\n",
    "for item in X_train:\n",
    "    length = len(item)\n",
    "    if length > padding_length:\n",
    "        length_count += 1\n",
    "print('Number of texts > word length: ' + str(length_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Ensure padded shape of dimension\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN LSTM model\n",
    "- We will be embedding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('GPU found')\n",
    "# else:\n",
    "#     print(\"No GPU found\")\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(unique_words_count, output_dim=100, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"RNN_LSTM.h5\", save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_test, y_test), callbacks=[model_checkpoint])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss: ' + str(loss) + '    ' + 'Accuracy: ' + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
