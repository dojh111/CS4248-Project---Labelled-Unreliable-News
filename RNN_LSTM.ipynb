{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM Text Classificaiton model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string                           # For removal of punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('drive/MyDrive/School Work/CS4248/News Labelling Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data into pd dataframes, data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total rows, Total Columns: (48854, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47952</th>\n",
       "      <td>4</td>\n",
       "      <td>Matt Szczur said he just did what anyone who w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29152</th>\n",
       "      <td>3</td>\n",
       "      <td>Wests Undermining of UN Paves Way for NATO Syr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>1</td>\n",
       "      <td>Saying that he makes a point to simply tune ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48590</th>\n",
       "      <td>4</td>\n",
       "      <td>Former Yuanta Financial Holding President Ma W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35200</th>\n",
       "      <td>3</td>\n",
       "      <td>Chasing Happiness: Ill Be Happy When...SSP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "47952  4  Matt Szczur said he just did what anyone who w...\n",
       "29152  3  Wests Undermining of UN Paves Way for NATO Syr...\n",
       "10926  1  Saying that he makes a point to simply tune ou...\n",
       "48590  4  Former Yuanta Financial Holding President Ma W...\n",
       "35200  3        Chasing Happiness: Ill Be Happy When...SSP "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file in\n",
    "train_path = './raw_data/fulltrain.csv'\n",
    "test_path = './raw_data/balancedtest.csv'\n",
    "df = pd.read_csv(train_path, header=None)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(df.shape))\n",
    "df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire: 14047\n",
      "Hoax: 6942\n",
      "Propaganda: 17870\n",
      "Reliable News: 9995\n",
      "3    17870\n",
      "1    14047\n",
      "4     9995\n",
      "2     6942\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((df[0] == label).sum()))\n",
    "print(df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows, Total Columns:       0                                                  1\n",
      "0     1  When so many actors seem content to churn out ...\n",
      "1     1   In what football insiders are calling an unex...\n",
      "2     1  In a freak accident following Game 3 of the N....\n",
      "3     1  North Koreas official news agency announced to...\n",
      "4     1  The former Alaska Governor Sarah Palin would b...\n",
      "...  ..                                                ...\n",
      "2995  4  The Air Force mistakenly gave rival companies ...\n",
      "2996  4  The United Nations climate chief on Friday cha...\n",
      "2997  4  River Plate midfielder Diego Buonanotte has un...\n",
      "2998  4  Lawmakers were on the brink Tuesday of exempti...\n",
      "2999  4  The Pentagon, which is processing bids on a ne...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>2</td>\n",
       "      <td>Taliban KILL At Least 20 Children And One Sold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1</td>\n",
       "      <td>In a dramatic narrative that could upstage thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1</td>\n",
       "      <td>A new poll released Wednesday revealed that p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>2</td>\n",
       "      <td>Judge Jeanine Compares Reaction Of Candidates ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>2</td>\n",
       "      <td>Wikileaks Assange On Clintons Russia Experts J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "1041  2  Taliban KILL At Least 20 Children And One Sold...\n",
       "500   1  In a dramatic narrative that could upstage thi...\n",
       "460   1   A new poll released Wednesday revealed that p...\n",
       "1360  2  Judge Jeanine Compares Reaction Of Candidates ...\n",
       "1481  2  Wikileaks Assange On Clintons Russia Experts J..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(test_df))\n",
    "test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire: 750\n",
      "Hoax: 750\n",
      "Propaganda: 750\n",
      "Reliable News: 750\n",
      "1    750\n",
      "2    750\n",
      "3    750\n",
      "4    750\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((test_df[0] == label).sum()))\n",
    "print(test_df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "- Removal of punctuation in Python Strings, [link](https://datagy.io/python-remove-punctuation-from-string/#:~:text=One%20of%20the%20easiest%20ways,maketrans()%20method.)\n",
    "- Can look at common name removal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of stopwords for use in preprocessing\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "# print(stopword_set)\n",
    "\n",
    "# Fold to lower case\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenise_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text, stopword_set):\n",
    "    # Split by whitespace\n",
    "    split_text = text.split()\n",
    "    new_tokens = []\n",
    "    for token in split_text:\n",
    "        if token in stopword_set:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    # Parse back into text\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "# Remove all punctuations - Affects words such as U.S.A etc\n",
    "# Removal of stop words has to be done prior to punctuation removal\n",
    "def remove_punctuation(text):\n",
    "    depunctuated_text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    return depunctuated_text\n",
    "\n",
    "# Prevent concatenation of statistics and names\n",
    "def replace_hyphens(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "# Combine all processes into a single preprocess text function to call on df\n",
    "def preprocess_text(text):\n",
    "    dehyphenated_text = replace_hyphens(text)\n",
    "    lowered_text = to_lower_case(dehyphenated_text)\n",
    "    initial_stopword_pass = remove_stopwords(lowered_text, stopword_set)\n",
    "    tokens = tokenise_text(initial_stopword_pass)\n",
    "    tokenised_text = ' '.join(tokens)\n",
    "    depunctuated_text = remove_punctuation(tokenised_text)\n",
    "    second_stopword_pass = remove_stopwords(depunctuated_text, stopword_set)\n",
    "    return second_stopword_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'usa days ago spent 1340 real'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test text preprocessing model\n",
    "test_string = \"I was down in the U.S.A a few days ago! Spent $1,340. But i'll be real, don't do it. Isn't it?\"\n",
    "print('Preprocessing test: ')\n",
    "preprocess_text(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all text in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning text...')\n",
    "df[1] = df[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning text...')\n",
    "test_df[1] = test_df[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "test_df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of unique words in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique words\n",
    "def unique_word_counter(texts):\n",
    "    count = Counter() # Dictionary type\n",
    "    # Access an entire string\n",
    "    for text in texts:\n",
    "        # Split each string into individual words separated by whitespace\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run counter\n",
    "counts = unique_word_counter(df[1])\n",
    "unique_words_count = len(counts)\n",
    "print('Number of unique words: ' + str(unique_words_count))\n",
    "print('Most Common Words:')\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets, convert into numpy format for Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[1].to_numpy()\n",
    "y_train = df[0].to_numpy()\n",
    "\n",
    "X_test = test_df[1].to_numpy()\n",
    "y_test = test_df[0].to_numpy()\n",
    "\n",
    "y_train = pd.get_dummies(df[0]).values\n",
    "y_test = pd.get_dummies(test_df[0]).values\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "# y_train = y_train - 1\n",
    "# y_test = y_test - 1\n",
    "# temp_y_train = []\n",
    "# for label in y_train:\n",
    "#     temp_y_train.append([label])\n",
    "# y_train = np.array(temp_y_train)\n",
    "# temp_y_test = []\n",
    "# for label in y_test:\n",
    "#     temp_y_test.append([label])\n",
    "# y_test = np.array(temp_y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise words into numbers\n",
    "- Each word will be assigned a specific number, according to how many unique words we have\n",
    "- Inspired from this [Youtube Video](https://www.youtube.com/watch?v=kxeyoyrf2cM&ab_channel=PythonEngineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each string is turned into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=unique_words_count)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences to a common length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine minimum number of words in a sequence 65218, \n",
    "\n",
    "padding_length = 1000\n",
    "\n",
    "length_count = 0\n",
    "for item in X_train:\n",
    "    length = len(item)\n",
    "    if length > padding_length:\n",
    "        length_count += 1\n",
    "print('Number of texts > word length: ' + str(length_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Ensure padded shape of dimension\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN LSTM model\n",
    "- We will be embedding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(unique_words_count, output_dim=100, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_test, y_test), callbacks=[model_checkpoint])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss: ' + str(loss) + '    ' + 'Accuracy: ' + str(accuracy))\n",
    "\n",
    "model.save('RNN_LSTM_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.read_csv(test_path, header=None)\n",
    "sample = new_test.sample(30)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning text...')\n",
    "sample[1] = sample[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "\n",
    "sample_X = sample[1].to_numpy()\n",
    "sample_y = sample[0].to_numpy()\n",
    "sample_y = sample_y - 1\n",
    "\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "actual_labels = []\n",
    "for label in sample_y:\n",
    "    actual_labels.append(classes[label])\n",
    "print(actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X = tokenizer.texts_to_sequences(sample_X)\n",
    "sample_X = pad_sequences(sample_X, maxlen=padding_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = []\n",
    "for test_instance in sample_X:\n",
    "    prediction = model.predict(test_instance)\n",
    "    predictions.append(classes[np.argmax(prediction)])\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
