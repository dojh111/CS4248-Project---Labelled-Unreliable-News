{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM Text Classificaiton model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string                           # For removal of punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('drive/MyDrive/CS4248')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data into pd dataframes, data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total rows, Total Columns: (48854, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>1</td>\n",
       "      <td>Following the ongoing power outage that has le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>2</td>\n",
       "      <td>88 US Generals Stand Together Against Hillary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32452</th>\n",
       "      <td>3</td>\n",
       "      <td>The NDAA: What They Dont Want You To KnowLily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41254</th>\n",
       "      <td>4</td>\n",
       "      <td>Chinese shares ended slightly higher on midday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29181</th>\n",
       "      <td>3</td>\n",
       "      <td>The Fall of Mainstream Media: When Propaganda ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "5756   1  Following the ongoing power outage that has le...\n",
       "19922  2  88 US Generals Stand Together Against Hillary ...\n",
       "32452  3  The NDAA: What They Dont Want You To KnowLily ...\n",
       "41254  4  Chinese shares ended slightly higher on midday...\n",
       "29181  3  The Fall of Mainstream Media: When Propaganda ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file in\n",
    "train_path = './raw_data/fulltrain.csv'\n",
    "test_path = './raw_data/balancedtest.csv'\n",
    "df = pd.read_csv(train_path, header=None)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(df.shape))\n",
    "df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire: 14047\n",
      "Hoax: 6942\n",
      "Propaganda: 17870\n",
      "Reliable News: 9995\n",
      "3    17870\n",
      "1    14047\n",
      "4     9995\n",
      "2     6942\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((df[0] == label).sum()))\n",
    "print(df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows, Total Columns:       0                                                  1\n",
      "0     1  When so many actors seem content to churn out ...\n",
      "1     1   In what football insiders are calling an unex...\n",
      "2     1  In a freak accident following Game 3 of the N....\n",
      "3     1  North Koreas official news agency announced to...\n",
      "4     1  The former Alaska Governor Sarah Palin would b...\n",
      "...  ..                                                ...\n",
      "2995  4  The Air Force mistakenly gave rival companies ...\n",
      "2996  4  The United Nations climate chief on Friday cha...\n",
      "2997  4  River Plate midfielder Diego Buonanotte has un...\n",
      "2998  4  Lawmakers were on the brink Tuesday of exempti...\n",
      "2999  4  The Pentagon, which is processing bids on a ne...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>4</td>\n",
       "      <td>Scientists reported Thursday they had develope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>3</td>\n",
       "      <td>The latest revelation regarding the Zika viru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>1</td>\n",
       "      <td>Former South Carolina Governor Mark Sanfords s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1</td>\n",
       "      <td>The Vatican has confirmed that while Pope Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>4</td>\n",
       "      <td>Consumers will have to divulge more personal i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "2519  4  Scientists reported Thursday they had develope...\n",
       "2010  3   The latest revelation regarding the Zika viru...\n",
       "635   1  Former South Carolina Governor Mark Sanfords s...\n",
       "474   1  The Vatican has confirmed that while Pope Fran...\n",
       "2759  4  Consumers will have to divulge more personal i..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(test_df))\n",
    "test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire: 750\n",
      "Hoax: 750\n",
      "Propaganda: 750\n",
      "Reliable News: 750\n",
      "1    750\n",
      "2    750\n",
      "3    750\n",
      "4    750\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((test_df[0] == label).sum()))\n",
    "print(test_df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of unique words in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique words\n",
    "def unique_word_counter(texts):\n",
    "    count = Counter() # Dictionary type\n",
    "    # Access an entire string\n",
    "    for text in texts:\n",
    "        # Split each string into individual words separated by whitespace\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 677571\n",
      "Most Common Words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1356038),\n",
       " ('to', 729997),\n",
       " ('of', 705981),\n",
       " ('and', 637368),\n",
       " ('a', 522053),\n",
       " ('in', 429077),\n",
       " ('that', 324277),\n",
       " ('is', 303340),\n",
       " ('for', 227190),\n",
       " ('on', 179343)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run counter\n",
    "counts = unique_word_counter(df[1])\n",
    "unique_words_count = len(counts)\n",
    "print('Number of unique words: ' + str(unique_words_count))\n",
    "print('Most Common Words:')\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets, convert into numpy format for Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(48854,) (3000,)\n",
      "(48854, 4) (3000, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = df[1].to_numpy()\n",
    "y_train = df[0].to_numpy()\n",
    "\n",
    "X_test = test_df[1].to_numpy()\n",
    "y_test = test_df[0].to_numpy()\n",
    "\n",
    "y_train = pd.get_dummies(df[0]).values\n",
    "y_test = pd.get_dummies(test_df[0]).values\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "# y_train = y_train - 1\n",
    "# y_test = y_test - 1\n",
    "# temp_y_train = []\n",
    "# for label in y_train:\n",
    "#     temp_y_train.append([label])\n",
    "# y_train = np.array(temp_y_train)\n",
    "# temp_y_test = []\n",
    "# for label in y_test:\n",
    "#     temp_y_test.append([label])\n",
    "# y_test = np.array(temp_y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise words into numbers\n",
    "- Each word will be assigned a specific number, according to how many unique words we have\n",
    "- Inspired from this [Youtube Video](https://www.youtube.com/watch?v=kxeyoyrf2cM&ab_channel=PythonEngineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\School Work\\Y3S2\\CS4248\\Project\\Sample Datasets\\RNN_LSTM.ipynb Cell 16'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/School%20Work/Y3S2/CS4248/Project/Sample%20Datasets/RNN_LSTM.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39m# Each string is turned into a sequence of integers\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/School%20Work/Y3S2/CS4248/Project/Sample%20Datasets/RNN_LSTM.ipynb#ch0000015?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(num_words\u001b[39m=\u001b[39munique_words_count)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/School%20Work/Y3S2/CS4248/Project/Sample%20Datasets/RNN_LSTM.ipynb#ch0000015?line=2'>3</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mfit_on_texts(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/School%20Work/Y3S2/CS4248/Project/Sample%20Datasets/RNN_LSTM.ipynb#ch0000015?line=4'>5</a>\u001b[0m X_train \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/School%20Work/Y3S2/CS4248/Project/Sample%20Datasets/RNN_LSTM.ipynb#ch0000015?line=5'>6</a>\u001b[0m X_test \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras_preprocessing\\text.py:222\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=219'>220</a>\u001b[0m     seq \u001b[39m=\u001b[39m text\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=220'>221</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=221'>222</a>\u001b[0m     seq \u001b[39m=\u001b[39m text_to_word_sequence(text,\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=222'>223</a>\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilters,\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=223'>224</a>\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlower,\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=224'>225</a>\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit)\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=225'>226</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m seq:\n\u001b[0;32m    <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=226'>227</a>\u001b[0m     \u001b[39mif\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_counts:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras_preprocessing\\text.py:63\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=59'>60</a>\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mtranslate(translate_map)\n\u001b[0;32m     <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=61'>62</a>\u001b[0m seq \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit(split)\n\u001b[1;32m---> <a href='file:///c%3A/Users/dojh1/AppData/Roaming/Python/Python39/site-packages/keras_preprocessing/text.py?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m seq \u001b[39mif\u001b[39;00m i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Each string is turned into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=unique_words_count)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences to a common length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_length = 1000\n",
    "\n",
    "length_count = 0\n",
    "for item in X_train:\n",
    "    length = len(item)\n",
    "    if length > padding_length:\n",
    "        length_count += 1\n",
    "print('Number of texts > word length: ' + str(length_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Ensure padded shape of dimension\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN LSTM model\n",
    "- We will be embedding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('GPU found')\n",
    "# else:\n",
    "#     print(\"No GPU found\")\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(unique_words_count, output_dim=100, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# change name here\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"RNN_LSTM.h5\", save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_test, y_test), callbacks=[model_checkpoint])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss: ' + str(loss) + '    ' + 'Accuracy: ' + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
