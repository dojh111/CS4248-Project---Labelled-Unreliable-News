{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM Text Classificaiton model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string                           # For removal of punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data into pd dataframes, data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file in\n",
    "train_path = './raw_data/fulltrain.csv'\n",
    "test_path = './raw_data/balancedtest.csv'\n",
    "df = pd.read_csv(train_path, header=None)\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(df.shape))\n",
    "df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((df[0] == label).sum()))\n",
    "print(df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "# Samples, number of columns, 0 = labels, column 1 = text\n",
    "print('Total rows, Total Columns: ' + str(test_df))\n",
    "test_df.sample(5) # Random sample values to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of labels for each task\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "label_numbers = [1,2,3,4]\n",
    "\n",
    "for label in label_numbers:\n",
    "    print(classes[label-1] + ': ' + str((test_df[0] == label).sum()))\n",
    "print(test_df[0].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "- Removal of punctuation in Python Strings, [link](https://datagy.io/python-remove-punctuation-from-string/#:~:text=One%20of%20the%20easiest%20ways,maketrans()%20method.)\n",
    "- Can look at common name removal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of stopwords for use in preprocessing\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "# print(stopword_set)\n",
    "\n",
    "# Fold to lower case\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenise_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text, stopword_set):\n",
    "    # Split by whitespace\n",
    "    split_text = text.split()\n",
    "    new_tokens = []\n",
    "    for token in split_text:\n",
    "        if token in stopword_set:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    # Parse back into text\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "# Remove all punctuations - Affects words such as U.S.A etc\n",
    "# Removal of stop words has to be done prior to punctuation removal\n",
    "def remove_punctuation(text):\n",
    "    depunctuated_text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    return depunctuated_text\n",
    "\n",
    "# Prevent concatenation of statistics and names\n",
    "def replace_hyphens(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "# Combine all processes into a single preprocess text function to call on df\n",
    "def preprocess_text(text):\n",
    "    dehyphenated_text = replace_hyphens(text)\n",
    "    lowered_text = to_lower_case(dehyphenated_text)\n",
    "    initial_stopword_pass = remove_stopwords(lowered_text, stopword_set)\n",
    "    tokens = tokenise_text(initial_stopword_pass)\n",
    "    tokenised_text = ' '.join(tokens)\n",
    "    depunctuated_text = remove_punctuation(tokenised_text)\n",
    "    second_stopword_pass = remove_stopwords(depunctuated_text, stopword_set)\n",
    "    return second_stopword_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text preprocessing model\n",
    "test_string = \"I was down in the U.S.A a few days ago! Spent $1,340. But i'll be real, don't do it. Isn't it?\"\n",
    "print('Preprocessing test: ')\n",
    "preprocess_text(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all text in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning text...')\n",
    "df[1] = df[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning text...')\n",
    "test_df[1] = test_df[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "test_df.sample(10) # Random sample values to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of unique words in the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique words\n",
    "def unique_word_counter(texts):\n",
    "    count = Counter() # Dictionary type\n",
    "    # Access an entire string\n",
    "    for text in texts:\n",
    "        # Split each string into individual words separated by whitespace\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run counter\n",
    "counts = unique_word_counter(df[1])\n",
    "unique_words_count = len(counts)\n",
    "print('Number of unique words: ' + str(unique_words_count))\n",
    "print('Most Common Words:')\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets, convert into numpy format for Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[1].to_numpy()\n",
    "y_train = df[0].to_numpy()\n",
    "y_train = y_train - 1\n",
    "X_test = test_df[1].to_numpy()\n",
    "y_test = test_df[0].to_numpy()\n",
    "y_test = y_test - 1\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise words into numbers\n",
    "- Each word will be assigned a specific number, according to how many unique words we have\n",
    "- Inspired from this [Youtube Video](https://www.youtube.com/watch?v=kxeyoyrf2cM&ab_channel=PythonEngineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each string is turned into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=unique_words_count)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences to a common length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine minimum number of words in a sequence 65218, \n",
    "\n",
    "length_count = 0\n",
    "for item in X_train:\n",
    "    length = len(item)\n",
    "    if length > 3000:\n",
    "        length_count += 1\n",
    "print('Number of texts > 5000 word length: ' + str(length_count))\n",
    "\n",
    "padding_length = 3000\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Ensure padded shape of dimension\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN LSTM model\n",
    "- We will be embedding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(unique_words_count, 200, input_length=padding_length))\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss: ' + str(loss) + '    ' + 'Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>3</td>\n",
       "      <td>Oxidative stress is a constant force in our e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>Responding to reports that President Obama is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>4</td>\n",
       "      <td>Dick Francis a champion jockey for the British...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>Missing Baby Boy Abandoned By Mother Dainesha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>Finally, a better way to produce food than far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>3</td>\n",
       "      <td>There are many reasons why antibiotics should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>2</td>\n",
       "      <td>[Watch] Pro-Invasion Racist Group Leader Calls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>3</td>\n",
       "      <td>Toxic flame retardant chemicals were found in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>3</td>\n",
       "      <td>A California father's desperate quest to find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1</td>\n",
       "      <td>The brave new world of social media torpedoed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>In a deepening humanitarian crisis, residents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear American Person: As many of you know, in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>4</td>\n",
       "      <td>'Robin Hood' (PG-13, 140 minutes): Dark and po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>4</td>\n",
       "      <td>Continental Airlines Inc. and one of its mecha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>3</td>\n",
       "      <td>Unsweetened, raw cocoa powder, the same stuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>3</td>\n",
       "      <td>The Obama administration, which regularly tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>4</td>\n",
       "      <td>SINGAPORE - Fears that quantitative easing by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>4</td>\n",
       "      <td>The Florida pastor who planned to burn Korans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>2</td>\n",
       "      <td>New Black Panther Leader Calls Police And Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>4</td>\n",
       "      <td>AMSTERDAM - A lawyer for Geert Wilders, a Dutc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>4</td>\n",
       "      <td>ftr akngonth acintsae onucus ho Yun-at has sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>4</td>\n",
       "      <td>The federal government will bear virtually the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>2</td>\n",
       "      <td>Hillary Clinton Campaign Raising $2.5 BILLION ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>4</td>\n",
       "      <td>In the Manhattan neighborhood of Elena Kagan's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>3</td>\n",
       "      <td>Big Pharma's pill pushers and natural remedy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>4</td>\n",
       "      <td>NEW YORK - New York City officials are studyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>3</td>\n",
       "      <td>In an effort to get more children to drink pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>After several days of controversy over whether...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>4</td>\n",
       "      <td>For the past year, I've been warning that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>3</td>\n",
       "      <td>Spinach is one of the most nutrient dense foo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "2066  3   Oxidative stress is a constant force in our e...\n",
       "67    1  Responding to reports that President Obama is ...\n",
       "2888  4  Dick Francis a champion jockey for the British...\n",
       "886   2  Missing Baby Boy Abandoned By Mother Dainesha ...\n",
       "110   1  Finally, a better way to produce food than far...\n",
       "2036  3   There are many reasons why antibiotics should...\n",
       "762   2  [Watch] Pro-Invasion Racist Group Leader Calls...\n",
       "2166  3   Toxic flame retardant chemicals were found in...\n",
       "1933  3   A California father's desperate quest to find...\n",
       "525   1  The brave new world of social media torpedoed ...\n",
       "123   1   In a deepening humanitarian crisis, residents...\n",
       "534   1  Dear American Person: As many of you know, in ...\n",
       "2600  4  'Robin Hood' (PG-13, 140 minutes): Dark and po...\n",
       "2828  4  Continental Airlines Inc. and one of its mecha...\n",
       "1699  3   Unsweetened, raw cocoa powder, the same stuff...\n",
       "1894  3   The Obama administration, which regularly tou...\n",
       "2371  4  SINGAPORE - Fears that quantitative easing by ...\n",
       "2680  4  The Florida pastor who planned to burn Korans ...\n",
       "1062  2  New Black Panther Leader Calls Police And Lear...\n",
       "2679  4  AMSTERDAM - A lawyer for Geert Wilders, a Dutc...\n",
       "2393  4  ftr akngonth acintsae onucus ho Yun-at has sig...\n",
       "2903  4  The federal government will bear virtually the...\n",
       "1116  2  Hillary Clinton Campaign Raising $2.5 BILLION ...\n",
       "2850  4  In the Manhattan neighborhood of Elena Kagan's...\n",
       "1559  3   Big Pharma's pill pushers and natural remedy ...\n",
       "2537  4  NEW YORK - New York City officials are studyin...\n",
       "2048  3   In an effort to get more children to drink pr...\n",
       "263   1  After several days of controversy over whether...\n",
       "2636  4  For the past year, I've been warning that the ...\n",
       "2040  3   Spinach is one of the most nutrient dense foo..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = pd.read_csv(test_path, header=None)\n",
    "sample = new_test.sample(30)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Preprocessing done!\n",
      "['Propaganda', 'Satire', 'Reliable News', 'Hoax', 'Satire', 'Propaganda', 'Hoax', 'Propaganda', 'Propaganda', 'Satire', 'Satire', 'Satire', 'Reliable News', 'Reliable News', 'Propaganda', 'Propaganda', 'Reliable News', 'Reliable News', 'Hoax', 'Reliable News', 'Reliable News', 'Reliable News', 'Hoax', 'Reliable News', 'Propaganda', 'Reliable News', 'Propaganda', 'Satire', 'Reliable News', 'Propaganda']\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning text...')\n",
    "sample[1] = sample[1].map(preprocess_text)\n",
    "print('Preprocessing done!')\n",
    "\n",
    "sample_X = sample[1].to_numpy()\n",
    "sample_y = sample[0].to_numpy()\n",
    "sample_y = sample_y - 1\n",
    "\n",
    "classes = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "actual_labels = []\n",
    "for label in sample_y:\n",
    "    actual_labels.append(classes[label])\n",
    "print(actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X = tokenizer.texts_to_sequences(sample_X)\n",
    "sample_X = pad_sequences(sample_X, maxlen=padding_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = []\n",
    "for test_instance in sample_X:\n",
    "    prediction = model.predict(test_instance)\n",
    "    predictions.append(classes[np.argmax(prediction)])\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21df0356f9123b9ffe2140fcee819cd2cf32077b8756437e0847ceff6279ac46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
